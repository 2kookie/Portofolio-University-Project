{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyek Klasifikasi gambar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "zip_ref = zipfile.ZipFile('rockpaperscissors.zip', 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dir = '/tmp/rockpaperscissors/rps-cv-images'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nimport shutil\\n# Potential extra class directories to remove\\nextra_classes = ['extra_class1', 'extra_class2']  # Replace with the actual names\\n\\n# Remove extra class directories if they exist\\nfor cls in extra_classes:\\n    cls_path = os.path.join(base_dir, cls)\\n    if os.path.exists(cls_path):\\n        shutil.rmtree(cls_path) \\n        \""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "import shutil\n",
    "# def\n",
    "extra_classes = ['extra_class1', 'extra_class2']  # \n",
    "\n",
    "# Remove extra class \n",
    "for cls in extra_classes:\n",
    "    cls_path = os.path.join(base_dir, cls)\n",
    "    if os.path.exists(cls_path):\n",
    "        shutil.rmtree(cls_path) \n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "image_count = len(glob.glob(base_dir + '/*/*.png'))\n",
    "print(image_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhidden_files = glob.glob(os.path.join(base_dir, \\'.*\\')) + glob.glob(os.path.join(base_dir, \\'*/*.*~\\')) + glob.glob(os.path.join(base_dir, \\'*/*.ini\\'))\\n\\nfor f in hidden_files:\\n    print(f\"Removing hidden file: {f}\")\\n    os.remove(f)\\n\\n# Now, list the contents of the directory again to confirm\\n!dir \\tmp\\rockpaperscissors\\rps-cv-images\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "hidden_files = glob.glob(os.path.join(base_dir, '.*')) + glob.glob(os.path.join(base_dir, '*/*.*~')) + glob.glob(os.path.join(base_dir, '*/*.ini'))\n",
    "\n",
    "for f in hidden_files:\n",
    "    print(f\"Removing hidden file: {f}\")\n",
    "    os.remove(f)\n",
    "\n",
    "# \n",
    "!dir \\tmp\\rockpaperscissors\\rps-cv-images\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Move train and val directories\\nfor dir_name in ['train', 'val']:\\n    dir_path = os.path.join(base_dir, dir_name)\\n    if os.path.exists(dir_path):\\n        new_location = os.path.join('c:/tmp/rockpaperscissors', dir_name)\\n        shutil.move(dir_path, new_location)\\n\\n# Move the README file\\nreadme_path = os.path.join(base_dir, 'README_rpc-cv-images.txt')\\nif os.path.exists(readme_path):\\n    new_location = 'c:/tmp/rockpaperscissors/README_rpc-cv-images.txt'\\n    shutil.move(readme_path, new_location)\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for dir_name in ['train', 'val']:\n",
    "    dir_path = os.path.join(base_dir, dir_name)\n",
    "    if os.path.exists(dir_path):\n",
    "        new_location = os.path.join('c:/tmp/rockpaperscissors', dir_name)\n",
    "        shutil.move(dir_path, new_location)\n",
    "\n",
    "# Move README file\n",
    "readme_path = os.path.join(base_dir, 'README_rpc-cv-images.txt')\n",
    "if os.path.exists(readme_path):\n",
    "    new_location = 'c:/tmp/rockpaperscissors/README_rpc-cv-images.txt'\n",
    "    shutil.move(readme_path, new_location)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1314 images belonging to 3 classes.\n",
      "Found 874 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='wrap',\n",
    "    validation_split=0.4 \n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.4\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(3, activation='softmax') \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.85):\n",
    "            print(\"\\nAkurasi telah mencapai >85%, training dihentikan!\")\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 - 33s - loss: 1.4544 - accuracy: 0.3805 - val_loss: 0.9946 - val_accuracy: 0.5875 - 33s/epoch - 1s/step\n",
      "Epoch 2/20\n",
      "25/25 - 22s - loss: 0.7984 - accuracy: 0.6688 - val_loss: 0.8828 - val_accuracy: 0.6875 - 22s/epoch - 866ms/step\n",
      "Epoch 3/20\n",
      "25/25 - 19s - loss: 0.4786 - accuracy: 0.8425 - val_loss: 0.2709 - val_accuracy: 0.9250 - 19s/epoch - 759ms/step\n",
      "Epoch 4/20\n",
      "25/25 - 22s - loss: 0.4487 - accuracy: 0.8286 - val_loss: 0.2496 - val_accuracy: 0.9187 - 22s/epoch - 881ms/step\n",
      "Epoch 5/20\n",
      "\n",
      "Akurasi telah mencapai >85%, training dihentikan!\n",
      "25/25 - 21s - loss: 0.2900 - accuracy: 0.8961 - val_loss: 0.2144 - val_accuracy: 0.9250 - 21s/epoch - 844ms/step\n"
     ]
    }
   ],
   "source": [
    "callbacks = myCallback()\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=25, \n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=5, \n",
    "    verbose=2,\n",
    "    callbacks=[callbacks]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 74, 74, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPooli  (None, 36, 36, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPooli  (None, 17, 17, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 36992)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               18940416  \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19035203 (72.61 MB)\n",
      "Trainable params: 19035203 (72.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from ipywidgets import FileUpload\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077d3bcfbc174cd5acc2ce6977771b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.jpg,.png,.jpeg', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "upload = FileUpload(accept='.jpg,.png,.jpeg', multiple=False)\n",
    "display(upload)\n",
    "\n",
    "def on_upload_change(change):\n",
    "    if not upload.value:\n",
    "        return  \n",
    "    \n",
    "    for uploaded_filename in upload.value:\n",
    "        content = upload.value[uploaded_filename]['content']\n",
    "      \n",
    "        img = image.load_img(io.BytesIO(content), target_size=(150,150))\n",
    "  \n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x /= 255.0\n",
    "\n",
    "        prediction = model.predict(x)\n",
    "        \n",
    "        print(uploaded_filename)\n",
    "        predicted_class = np.argmax(prediction)  \n",
    "        classes = ['rock', 'paper', 'scissors']\n",
    "        print('Predicted class:', classes[predicted_class])\n",
    "\n",
    "upload.observe(on_upload_change, names='_counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paper': 0, 'rock': 1, 'scissors': 2}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
